import os
import chromadb
import tempfile
import tiktoken
import pandas as pd
import PyPDF2
from PIL import Image, UnidentifiedImageError
import io
from langchain_community.document_loaders import PyPDFLoader
from langchain.chains import RetrievalQA
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.chat_models import ChatOpenAI
from langchain.schema import Document
from shiny import App, ui, render, reactive  # ‚úÖ Shiny for Python

####################################################################################
### Initialize and define functions. 
### ChatGPT-4, OpenAI embeddings, and ChromaDB are used.
####################################################################################

# ‚úÖ Set OpenAI API Key
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("OpenAI API Key is missing! Set OPENAI_API_KEY as an environment variable.")

# ‚úÖ Initialize OpenAI Embeddings
embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)

# ‚úÖ Writable temporary directory for ChromaDB
CHROMA_DB_DIR = tempfile.mkdtemp()

# ‚úÖ Reset ChromaDB (but not on every upload)
def reset_chromadb():
    """Clears ChromaDB and reinitializes it with a clean slate."""
    global chroma_db

    try:
        if 'chroma_db' in globals() and chroma_db is not None:
            chroma_db.delete(ids=None)
            chroma_db.persist()
            del chroma_db
            print("üóëÔ∏è ChromaDB records cleared.")
    except Exception as e:
        print(f"‚ö†Ô∏è Warning: Could not clear ChromaDB properly: {e}")

    chroma_db = Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embeddings)
    print(f"ChromaDB reset. Storage location: {CHROMA_DB_DIR}")

# ‚úÖ Initialize ChromaDB
reset_chromadb()

# ‚úÖ LLM for Question-Answering (limits to keep costs down)
llm = ChatOpenAI(model_name="gpt-4", temperature=0, openai_api_key=OPENAI_API_KEY, max_tokens=500)

# ‚úÖ Optimized Retriever (Fetches top 6 relevant chunks)
retriever = chroma_db.as_retriever(search_kwargs={"k": 6})

# ‚úÖ QA Chain with Retriever
qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

# ‚úÖ Function to count tokens before sending to GPT-4
def count_tokens(text):
    encoding = tiktoken.get_encoding("cl100k_base")  # GPT-4's tokenizer
    return len(encoding.encode(text))

# ‚úÖ Optimized PDF Processing for Multiple PDFs
def process_pdf(file_path):
    """Extracts text from PDFs, splits it into chunks, and indexes it in ChromaDB."""
    try:
        loader = PyPDFLoader(file_path)
        pages = loader.load()
        file_name = os.path.basename(file_path)  # ‚úÖ Extract filename
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50, separators=["\n\n", "\n", " ", ""])
        docs = text_splitter.split_documents(pages)

        # ‚úÖ Attach filename as metadata to each chunk
        for doc in docs:
            doc.metadata["source"] = file_name  

        # ‚úÖ Add text-based chunks to ChromaDB
        chroma_db.add_documents(docs)

        print(f"‚úÖ Indexed {len(docs)} text chunks from {file_name}.")
        return docs
    except Exception as e:
        print(f"‚ùå Error processing {file_path}: {e}")
        return []

#############################################################################################################
### ‚úÖ Start of UI Layout
##############################################################################################################
app_ui = ui.page_fluid(
    ui.h2("üìÑ AI-Powered Multi-PDF Analyzer"),
    ui.h6("RAG-based LLM. Prompts will impact retrieval quality."),
    ui.h6("Use document section titles for better results."),
    ui.h6("Avoid large files to reduce processing costs."),
    ui.h6("Technical papers (< 80 pages) are ideal test cases."),
    ui.h6("This version retrieves fewer chunks to keep costs low."),
    ui.h6("If responses seem incomplete, the enterprise version can scale."),
    ui.h6("‚ö†Ô∏è Known issue: Re-uploading after first upload may cause issues. Restart the app if needed."),
    ui.input_file("file", "Upload PDF Documents", multiple=True, accept=[".pdf"]),  # ‚úÖ Supports multiple files
    ui.input_text("query", "Ask a question about the documents"),
    ui.input_action_button("ask", "Ask AI"),
    ui.output_text("result_text")
)

# ‚úÖ Server Logic
def server(input, output, session):
    """Handles file uploads and AI interactions"""

    @reactive.effect
    @reactive.event(input.file)
    def handle_file_upload():
        """Processes multiple uploaded PDF files and indexes them in ChromaDB."""
        file_info = input.file()
        if not file_info:
            return

        new_docs = []
        for file in file_info:
            file_path = file["datapath"]
            docs = process_pdf(file_path)  # ‚úÖ Process each PDF
            new_docs.extend(docs)  # ‚úÖ Store all extracted documents

        if new_docs:
            print(f"‚úÖ Indexed {len(new_docs)} chunks from {len(file_info)} PDFs into ChromaDB.")
        else:
            print("‚ùå No valid PDFs processed.")

    answer_text = reactive.value("")

    @reactive.effect
    @reactive.event(input.ask)
    def generate_response():
        """Handles GPT-4 response based on indexed PDF content."""
        query = input.query()
        if not query:
            print("‚ùå No query provided.")
            answer_text.set("Please enter a valid question.")
            return

        # ‚úÖ Ensure ChromaDB has indexed data before querying
        if chroma_db._collection.count() == 0:
            print("‚ùå No PDF uploaded. Please upload a file first.")
            answer_text.set("No PDFs uploaded. Please upload files before asking questions.")
            return

        print(f"üìù Query received: {query}")

        try:
            # ‚úÖ Retrieve relevant documents
            retrieved_docs = retriever.get_relevant_documents(query)

            # ‚úÖ Combine retrieved content with document source names
            combined_text = "\n\n".join(
                [f"üìÑ Source: {doc.metadata.get('source', 'Unknown')}\n{doc.page_content}" for doc in retrieved_docs]
            )

            # ‚úÖ Check token count before sending to GPT-4
            total_tokens = count_tokens(query + combined_text)
            print(f"üî¢ Total tokens: {total_tokens}")

            if total_tokens > 7500:  # Keep a buffer below 8192
                answer_text.set("‚ö†Ô∏è Query is too long. Try asking a more specific question.")
                return

            # ‚úÖ Retrieve context and query GPT-4
            result = qa_chain.invoke(query)
            result_text = result["result"] if isinstance(result, dict) and "result" in result else str(result)
            answer_text.set(result_text if result_text else "No relevant information found.")

        except Exception as e:
            print(f"‚ùå Error retrieving answer: {e}")
            answer_text.set("Error retrieving response.")

    @render.text
    def result_text():
        return answer_text.get() if answer_text.get() else "No response yet."

# ‚úÖ Run Shiny App
app = App(app_ui, server)
